{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk \nnltk.download('gutenberg')\nfrom nltk.corpus import gutenberg\nimport pandas as pd\n\ndata = gutenberg.raw('shakespeare-hamlet.txt')\n##save to file\n\nwith open ('hamlet.txt','w') as file:\n    file.write(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:32:22.837614Z","iopub.execute_input":"2025-07-08T09:32:22.837785Z","iopub.status.idle":"2025-07-08T09:32:26.862428Z","shell.execute_reply.started":"2025-07-08T09:32:22.837768Z","shell.execute_reply":"2025-07-08T09:32:26.861651Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package gutenberg to /usr/share/nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"## Data Preprocessing\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n##load The Dataset\nwith open('hamlet.txt','r') as file:\n    text = file.read().lower()\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text])\ntotal_words = len(tokenizer.word_index)+1\ntotal_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:37:42.989131Z","iopub.execute_input":"2025-07-08T09:37:42.989857Z","iopub.status.idle":"2025-07-08T09:37:43.011659Z","shell.execute_reply.started":"2025-07-08T09:37:42.989833Z","shell.execute_reply":"2025-07-08T09:37:43.011096Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"4818"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tokenizer.word_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:37:52.849209Z","iopub.execute_input":"2025-07-08T09:37:52.849501Z","iopub.status.idle":"2025-07-08T09:37:52.864522Z","shell.execute_reply.started":"2025-07-08T09:37:52.849453Z","shell.execute_reply":"2025-07-08T09:37:52.863825Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'the': 1,\n 'and': 2,\n 'to': 3,\n 'of': 4,\n 'i': 5,\n 'you': 6,\n 'a': 7,\n 'my': 8,\n 'it': 9,\n 'in': 10,\n 'that': 11,\n 'ham': 12,\n 'is': 13,\n 'not': 14,\n 'his': 15,\n 'this': 16,\n 'with': 17,\n 'your': 18,\n 'but': 19,\n 'for': 20,\n 'me': 21,\n 'lord': 22,\n 'as': 23,\n 'what': 24,\n 'he': 25,\n 'be': 26,\n 'so': 27,\n 'him': 28,\n 'haue': 29,\n 'king': 30,\n 'will': 31,\n 'no': 32,\n 'our': 33,\n 'we': 34,\n 'on': 35,\n 'are': 36,\n 'if': 37,\n 'all': 38,\n 'then': 39,\n 'shall': 40,\n 'by': 41,\n 'thou': 42,\n 'come': 43,\n 'or': 44,\n 'hamlet': 45,\n 'good': 46,\n 'do': 47,\n 'hor': 48,\n 'her': 49,\n 'let': 50,\n 'now': 51,\n 'thy': 52,\n 'how': 53,\n 'more': 54,\n 'they': 55,\n 'from': 56,\n 'enter': 57,\n 'at': 58,\n 'was': 59,\n 'oh': 60,\n 'like': 61,\n 'most': 62,\n 'there': 63,\n 'well': 64,\n 'know': 65,\n 'selfe': 66,\n 'would': 67,\n 'them': 68,\n 'loue': 69,\n 'may': 70,\n \"'tis\": 71,\n 'vs': 72,\n 'sir': 73,\n 'qu': 74,\n 'which': 75,\n 'did': 76,\n 'why': 77,\n 'laer': 78,\n 'giue': 79,\n 'thee': 80,\n 'ile': 81,\n 'must': 82,\n 'hath': 83,\n 'ophe': 84,\n 'speake': 85,\n 'out': 86,\n 'make': 87,\n 'should': 88,\n 'where': 89,\n 'too': 90,\n 'an': 91,\n 'am': 92,\n 'such': 93,\n 'say': 94,\n 'when': 95,\n 'vpon': 96,\n 'father': 97,\n 'doe': 98,\n 'very': 99,\n 'pol': 100,\n 'go': 101,\n 'their': 102,\n 'one': 103,\n 'man': 104,\n 'see': 105,\n 'some': 106,\n 'heere': 107,\n 'had': 108,\n 'heauen': 109,\n 'time': 110,\n 'mine': 111,\n 'these': 112,\n 'she': 113,\n 'much': 114,\n 'tell': 115,\n 'rosin': 116,\n 'thinke': 117,\n 'play': 118,\n 'thus': 119,\n 'horatio': 120,\n 'who': 121,\n 'mother': 122,\n 'queene': 123,\n 'night': 124,\n 'o': 125,\n 'polon': 126,\n 'yet': 127,\n 'vp': 128,\n 'death': 129,\n 'laertes': 130,\n 'againe': 131,\n 'can': 132,\n 'both': 133,\n \"th'\": 134,\n 'soule': 135,\n 'take': 136,\n 'life': 137,\n 'nor': 138,\n 'heare': 139,\n 'mar': 140,\n 'looke': 141,\n 'owne': 142,\n 'could': 143,\n 'heart': 144,\n 'dead': 145,\n 'might': 146,\n 'made': 147,\n 'clo': 148,\n 'hast': 149,\n 'downe': 150,\n 'pray': 151,\n 'ophelia': 152,\n 'nothing': 153,\n 'away': 154,\n 'whose': 155,\n 'doth': 156,\n 'other': 157,\n 'cannot': 158,\n 'leaue': 159,\n 'indeed': 160,\n 'into': 161,\n 'nay': 162,\n 'god': 163,\n 'head': 164,\n 'were': 165,\n 'matter': 166,\n 'thing': 167,\n 'hold': 168,\n 'day': 169,\n 'world': 170,\n 'nature': 171,\n 'neuer': 172,\n 'comes': 173,\n 'done': 174,\n 'exeunt': 175,\n 'call': 176,\n 'two': 177,\n 'true': 178,\n 'though': 179,\n 'sweet': 180,\n 'put': 181,\n 'set': 182,\n 'ghost': 183,\n 'euen': 184,\n 'earth': 185,\n 'feare': 186,\n 'madnesse': 187,\n 'mad': 188,\n 'seene': 189,\n 'eyes': 190,\n 'against': 191,\n 'faire': 192,\n 'denmarke': 193,\n 'those': 194,\n \"o're\": 195,\n 'polonius': 196,\n 'deere': 197,\n 'fathers': 198,\n 'sonne': 199,\n 'poore': 200,\n 'himselfe': 201,\n 'follow': 202,\n 'guild': 203,\n 'england': 204,\n 'friends': 205,\n 'once': 206,\n 'hand': 207,\n 'shew': 208,\n 'about': 209,\n \"i'th'\": 210,\n 'off': 211,\n 'within': 212,\n 'till': 213,\n 'great': 214,\n 'meanes': 215,\n 'words': 216,\n 'players': 217,\n 'exit': 218,\n 'part': 219,\n 'still': 220,\n 'does': 221,\n 'hee': 222,\n 'osr': 223,\n 'long': 224,\n 'before': 225,\n 'beleeue': 226,\n 'any': 227,\n 'old': 228,\n 'thoughts': 229,\n 'first': 230,\n 'eare': 231,\n 'keepe': 232,\n 'goe': 233,\n 'end': 234,\n 'guildensterne': 235,\n 'welcome': 236,\n 'while': 237,\n 'art': 238,\n 'noble': 239,\n 'body': 240,\n 'bee': 241,\n 'daughter': 242,\n 'speech': 243,\n 'makes': 244,\n \"there's\": 245,\n 'sword': 246,\n 'stand': 247,\n 'liue': 248,\n \"that's\": 249,\n 'farewell': 250,\n 'kin': 251,\n 'ere': 252,\n 'marry': 253,\n 'betweene': 254,\n 'many': 255,\n 'since': 256,\n 'watch': 257,\n \"ha's\": 258,\n 'therefore': 259,\n 'question': 260,\n 'thought': 261,\n 'heard': 262,\n 'spirit': 263,\n 'eye': 264,\n 'better': 265,\n 'thine': 266,\n 'tongue': 267,\n 'drinke': 268,\n 'youth': 269,\n 'sent': 270,\n 'graue': 271,\n 'rest': 272,\n 'bed': 273,\n 'last': 274,\n 'same': 275,\n 'marke': 276,\n 'gone': 277,\n 'without': 278,\n 'state': 279,\n \"is't\": 280,\n 'goes': 281,\n 'fortinbras': 282,\n 'vse': 283,\n 'grace': 284,\n 'euer': 285,\n 'finde': 286,\n 'gertrude': 287,\n 'beare': 288,\n 'little': 289,\n 'breath': 290,\n \"wee'l\": 291,\n 'saw': 292,\n 'beene': 293,\n 'none': 294,\n 'vertue': 295,\n 'else': 296,\n 'said': 297,\n 'after': 298,\n 'reynol': 299,\n 'cause': 300,\n 'forme': 301,\n 'something': 302,\n 'ayre': 303,\n 'farre': 304,\n 'selues': 305,\n 'purpose': 306,\n 'further': 307,\n 'reason': 308,\n 'friend': 309,\n 'madam': 310,\n 'remember': 311,\n 'faith': 312,\n 'gentlemen': 313,\n 'word': 314,\n 'foule': 315,\n 'winde': 316,\n 'meane': 317,\n 'bring': 318,\n 'fit': 319,\n 'blood': 320,\n 'helpe': 321,\n 'honest': 322,\n 'stay': 323,\n \"in't\": 324,\n 'being': 325,\n 'fire': 326,\n 'things': 327,\n \"what's\": 328,\n 'newes': 329,\n 'best': 330,\n 'kinde': 331,\n 'excellent': 332,\n 'each': 333,\n 'sleepe': 334,\n 'way': 335,\n 'please': 336,\n 'free': 337,\n 'reuenge': 338,\n 'villaine': 339,\n 'right': 340,\n 'ha': 341,\n 'passion': 342,\n 'rosincrance': 343,\n 'dost': 344,\n 'verie': 345,\n 'barn': 346,\n 'marcellus': 347,\n 'men': 348,\n 'peace': 349,\n 'together': 350,\n 'full': 351,\n 'voyce': 352,\n 'oft': 353,\n 'greefe': 354,\n \"'twere\": 355,\n 'late': 356,\n 'businesse': 357,\n 'doubt': 358,\n 'alone': 359,\n 'minde': 360,\n 'heauens': 361,\n 'face': 362,\n 'hell': 363,\n 'ye': 364,\n 'second': 365,\n 'iudgement': 366,\n 'giuen': 367,\n 'command': 368,\n 'action': 369,\n \"let's\": 370,\n 'murther': 371,\n 'guil': 372,\n 'lady': 373,\n 'fortune': 374,\n 'mee': 375,\n 'pyrrhus': 376,\n 'answer': 377,\n 'get': 378,\n 'thankes': 379,\n 'goodnight': 380,\n 'eares': 381,\n 'breake': 382,\n 'hora': 383,\n 'strange': 384,\n 'young': 385,\n 'walke': 386,\n 'brothers': 387,\n 'seeme': 388,\n 'name': 389,\n 'fellow': 390,\n 'act': 391,\n 'hands': 392,\n 'armes': 393,\n 'deare': 394,\n 'neere': 395,\n 'phrase': 396,\n 'draw': 397,\n 'gho': 398,\n 'alas': 399,\n 'ought': 400,\n 'offence': 401,\n 'sweare': 402,\n 'worke': 403,\n 'gentleman': 404,\n 'fine': 405,\n 'three': 406,\n 'barnardo': 407,\n 'fran': 408,\n 'ground': 409,\n 'sight': 410,\n 'sit': 411,\n 'maiesty': 412,\n 'pale': 413,\n \"on't\": 414,\n 'fell': 415,\n 'lost': 416,\n 'soft': 417,\n 'power': 418,\n 'yong': 419,\n 'duty': 420,\n 'whole': 421,\n 'woe': 422,\n 'ioy': 423,\n 'wife': 424,\n 'came': 425,\n 'queen': 426,\n 'seeke': 427,\n 'common': 428,\n 'seemes': 429,\n 'blacke': 430,\n 'kings': 431,\n 'teares': 432,\n 'top': 433,\n 'fashion': 434,\n 'deed': 435,\n 'euery': 436,\n 'light': 437,\n 'custome': 438,\n 'borne': 439,\n 'wilt': 440,\n 'hither': 441,\n 'lay': 442,\n 'another': 443,\n 'ouer': 444,\n 'age': 445,\n 'thousand': 446,\n 'fall': 447,\n 'lye': 448,\n 'conscience': 449,\n 'husband': 450,\n 'bar': 451,\n 'lookes': 452,\n 'charge': 453,\n 'knowne': 454,\n 'law': 455,\n 'bin': 456,\n 'sound': 457,\n 'sister': 458,\n 'memory': 459,\n 'brother': 460,\n 'beseech': 461,\n 'lesse': 462,\n 'dust': 463,\n 'through': 464,\n 'shewes': 465,\n 'desire': 466,\n 'obey': 467,\n 'woman': 468,\n 'almost': 469,\n 'grow': 470,\n 'here': 471,\n 'shame': 472,\n 'giues': 473,\n \"too't\": 474,\n 'takes': 475,\n 'table': 476,\n 'sure': 477,\n 'musicke': 478,\n 'letters': 479,\n 'hamlets': 480,\n 'hope': 481,\n 'receiue': 482,\n 'maiestie': 483,\n 'thanke': 484,\n 'gaue': 485,\n 'bad': 486,\n 'wee': 487,\n 'ore': 488,\n 'noise': 489,\n 'times': 490,\n 'cold': 491,\n 'bid': 492,\n 'dane': 493,\n 'place': 494,\n 'peece': 495,\n 'buried': 496,\n 'cast': 497,\n 'hot': 498,\n 'list': 499,\n 'wrong': 500,\n 'sea': 501,\n 'truth': 502,\n 'sayes': 503,\n 'season': 504,\n 'gracious': 505,\n 'dumbe': 506,\n 'loues': 507,\n 'sorrow': 508,\n 'marriage': 509,\n 'writ': 510,\n 'mouth': 511,\n 'pardon': 512,\n 'note': 513,\n 'backe': 514,\n 'lordship': 515,\n 'mothers': 516,\n 'beard': 517,\n 'fare': 518,\n 'seruice': 519,\n 'withall': 520,\n 'maid': 521,\n 'enough': 522,\n 'effect': 523,\n 'double': 524,\n 'neither': 525,\n 'false': 526,\n 'vnderstand': 527,\n 'circumstance': 528,\n 'foole': 529,\n 'vowes': 530,\n 'keepes': 531,\n 'shape': 532,\n 'dayes': 533,\n 'fat': 534,\n 'crowne': 535,\n 'wits': 536,\n 'damned': 537,\n 'ho': 538,\n 'needs': 539,\n 'touch': 540,\n 'ranke': 541,\n 'generall': 542,\n 'moue': 543,\n 'home': 544,\n 'ill': 545,\n 'round': 546,\n 'fortunes': 547,\n 'laugh': 548,\n 'yours': 549,\n \"he's\": 550,\n 'honor': 551,\n 'begin': 552,\n 'anon': 553,\n 'proofe': 554,\n 'gods': 555,\n 'quicke': 556,\n 'dangerous': 557,\n 'christian': 558,\n 'danish': 559,\n 'poyson': 560,\n 'begge': 561,\n 'wager': 562,\n \"drown'd\": 563,\n 'water': 564,\n 'scull': 565,\n 'houre': 566,\n 'twelue': 567,\n 'quiet': 568,\n 'course': 569,\n 'sometimes': 570,\n 'march': 571,\n 'look': 572,\n 'norwey': 573,\n 'particular': 574,\n 'land': 575,\n \"do's\": 576,\n 'vnto': 577,\n 'speak': 578,\n 'spirits': 579,\n 'cocke': 580,\n 'guilty': 581,\n \"'gainst\": 582,\n 'wholsome': 583,\n 'lords': 584,\n 'kingdome': 585,\n 'freely': 586,\n 'dreame': 587,\n 'told': 588,\n 'loose': 589,\n 'dread': 590,\n 'returne': 591,\n 'france': 592,\n 'confesse': 593,\n 'dye': 594,\n 'visage': 595,\n 'truly': 596,\n 'bound': 597,\n 'prythee': 598,\n 'health': 599,\n 'flesh': 600,\n 'fie': 601,\n 'beast': 602,\n 'discourse': 603,\n 'longer': 604,\n 'wicked': 605,\n 'disposition': 606,\n 'report': 607,\n 'teach': 608,\n 'forth': 609,\n 'thinkes': 610,\n 'tis': 611,\n 'yes': 612,\n 'countenance': 613,\n 'perchance': 614,\n 'warrant': 615,\n 'silence': 616,\n 'perhaps': 617,\n 'wisedome': 618,\n 'blessing': 619,\n 'dull': 620,\n 'mans': 621,\n 'audience': 622,\n \"you'l\": 623,\n 'making': 624,\n \"damn'd\": 625,\n 'soules': 626,\n 'cries': 627,\n 'desperate': 628,\n 'shalt': 629,\n 'prison': 630,\n 'went': 631,\n 'naturall': 632,\n 'holds': 633,\n 'sodaine': 634,\n 'adue': 635,\n 'braine': 636,\n 'knaue': 637,\n 'point': 638,\n 'vnder': 639,\n 'mercy': 640,\n 'lacke': 641,\n \"heere's\": 642,\n \"in's\": 643,\n 'tooke': 644,\n 'arme': 645,\n 'brought': 646,\n 'dutie': 647,\n 'found': 648,\n 'whereon': 649,\n 'commission': 650,\n 'passe': 651,\n 'vilde': 652,\n 'short': 653,\n 'try': 654,\n 'behinde': 655,\n 'presently': 656,\n 'slaue': 657,\n 'saue': 658,\n 'ambition': 659,\n 'sing': 660,\n 'themselues': 661,\n 'braines': 662,\n \"'twas\": 663,\n 'onely': 664,\n 'french': 665,\n 'treason': 666,\n 'morrow': 667,\n 'conceit': 668,\n 'drowne': 669,\n 'fellowes': 670,\n 'hoa': 671,\n 'patience': 672,\n 'halfe': 673,\n 'yeare': 674,\n 'forgot': 675,\n 'hence': 676,\n \"doo't\": 677,\n \"e'ene\": 678,\n 'slaine': 679,\n 'sense': 680,\n 'buriall': 681,\n 'alexander': 682,\n 'osricke': 683,\n 'carriages': 684,\n 'foyles': 685,\n 'hit': 686,\n 'tragedie': 687,\n 'sicke': 688,\n 'meet': 689,\n 'twice': 690,\n \"'twill\": 691,\n 'nights': 692,\n 'starre': 693,\n \"t'\": 694,\n 'figure': 695,\n 'wonder': 696,\n 'warlike': 697,\n 'cannon': 698,\n 'toward': 699,\n 'image': 700,\n 'norway': 701,\n \"seal'd\": 702,\n 'lands': 703,\n 'stood': 704,\n \"return'd\": 705,\n 'strong': 706,\n 'motiue': 707,\n 'ease': 708,\n 'treasure': 709,\n 'stop': 710,\n 'violence': 711,\n 'trumpet': 712,\n 'whether': 713,\n 'heerein': 714,\n 'wherein': 715,\n 'impart': 716,\n 'morning': 717,\n 'attendant': 718,\n 'brow': 719,\n 'discretion': 720,\n 'followes': 721,\n 'frame': 722,\n 'cornelius': 723,\n 'giuing': 724,\n 'commend': 725,\n \"would'st\": 726,\n 'natiue': 727,\n 'fauour': 728,\n 'hang': 729,\n 'colour': 730,\n 'show': 731,\n 'fault': 732,\n 'throw': 733,\n 'lose': 734,\n 'gentle': 735,\n 'sits': 736,\n 'growes': 737,\n 'visit': 738,\n 'growne': 739,\n 'vnkle': 740,\n 'left': 741,\n 'speed': 742,\n 'incestuous': 743,\n 'glad': 744,\n 'forget': 745,\n 'thrift': 746,\n \"arm'd\": 747,\n 'thrice': 748,\n 'length': 749,\n 'kept': 750,\n 'knew': 751,\n 'answere': 752,\n 'honour': 753,\n 'wide': 754,\n 'force': 755,\n 'affection': 756,\n 'shot': 757,\n 'danger': 758,\n 'moone': 759,\n 'safety': 760,\n 'lies': 761,\n 'buy': 762,\n 'aboue': 763,\n 'humbly': 764,\n 'tend': 765,\n 'ist': 766,\n 'bloud': 767,\n 'heate': 768,\n 'bones': 769,\n 'base': 770,\n 'horrible': 771,\n 'imagination': 772,\n 'direct': 773,\n 'lend': 774,\n 'hearing': 775,\n 'certaine': 776,\n 'fast': 777,\n 'house': 778,\n 'tale': 779,\n 'start': 780,\n 'vnnaturall': 781,\n \"it's\": 782,\n 'sleeping': 783,\n 'wit': 784,\n 'gifts': 785,\n 'seeming': 786,\n 'wil': 787,\n 'court': 788,\n 'cursed': 789,\n 'instant': 790,\n 'bosome': 791,\n 'distracted': 792,\n 'yea': 793,\n 'past': 794,\n 'booke': 795,\n 'think': 796,\n 'secret': 797,\n 'already': 798,\n 'stage': 799,\n 'seeing': 800,\n 'fingers': 801,\n 'reynoldo': 802,\n 'drift': 803,\n 'liberty': 804,\n 'closes': 805,\n 'consequence': 806,\n 'chamber': 807,\n \"turn'd\": 808,\n 'extasie': 809,\n 'violent': 810,\n 'meant': 811,\n 'hide': 812,\n 'whom': 813,\n 'rather': 814,\n 'white': 815,\n 'awhile': 816,\n 'idle': 817,\n 'thence': 818,\n 'bene': 819,\n 'foure': 820,\n 'honestie': 821,\n 'either': 822,\n 'count': 823,\n 'dreames': 824,\n 'bodies': 825,\n 'comming': 826,\n 'shal': 827,\n 'player': 828,\n 'flourish': 829,\n 'asse': 830,\n 'heauy': 831,\n 'masters': 832,\n '1': 833,\n 'others': 834,\n 'modestie': 835,\n 'cunning': 836,\n \"lou'd\": 837,\n 'horse': 838,\n 'priam': 839,\n 'new': 840,\n 'sleepes': 841,\n 'hecuba': 842,\n 'mortall': 843,\n 'weepe': 844,\n 'pate': 845,\n 'diuell': 846,\n 'blame': 847,\n 'flye': 848,\n 'turne': 849,\n 'beautie': 850,\n 'acte': 851,\n 'nunnery': 852,\n 'snow': 853,\n 'rose': 854,\n 'quite': 855,\n 'ladies': 856,\n 'wretched': 857,\n 'send': 858,\n 'weare': 859,\n \"kill'd\": 860,\n 'kill': 861,\n 'maker': 862,\n 'prologue': 863,\n 'shortly': 864,\n 'begun': 865,\n 'lights': 866,\n 'heeles': 867,\n 'doore': 868,\n 'meete': 869,\n 'stronger': 870,\n 'whereto': 871,\n 'next': 872,\n 'messenger': 873,\n 'choose': 874,\n 'bore': 875,\n 'spade': 876,\n 'gallowes': 877,\n 'sings': 878,\n 'rites': 879,\n \"woo't\": 880,\n 'cup': 881,\n 'vnfold': 882,\n 'bitter': 883,\n 'guard': 884,\n 'mouse': 885,\n \"appear'd\": 886,\n 'saies': 887,\n 'touching': 888,\n 'along': 889,\n 'appeare': 890,\n 'beating': 891,\n 'offended': 892,\n 'ambitious': 893,\n 'iust': 894,\n 'knowes': 895,\n 'subiect': 896,\n 'sore': 897,\n 'least': 898,\n 'valiant': 899,\n 'recouer': 900,\n 'termes': 901,\n 'maine': 902,\n 'loe': 903,\n 'offer': 904,\n 'present': 905,\n 'dew': 906,\n 'high': 907,\n 'hill': 908,\n 'aduice': 909,\n 'consent': 910,\n 'scena': 911,\n 'greene': 912,\n 'hearts': 913,\n 'wisest': 914,\n 'remembrance': 915,\n 'funerall': 916,\n 'delight': 917,\n 'affaire': 918,\n 'thinking': 919,\n 'voltemand': 920,\n 'vncle': 921,\n 'heartily': 922,\n 'bend': 923,\n 'bow': 924,\n 'cosin': 925,\n 'clouds': 926,\n 'sun': 927,\n 'liues': 928,\n 'passing': 929,\n 'formes': 930,\n 'vnderstanding': 931,\n 'sence': 932,\n 'coarse': 933,\n 'dyed': 934,\n 'beares': 935,\n 'wittenberg': 936,\n 'courtier': 937,\n 'louing': 938,\n 'smiling': 939,\n 'manet': 940,\n 'fixt': 941,\n 'flat': 942,\n 'vses': 943,\n 'married': 944,\n 'hercules': 945,\n 'salt': 946,\n 'change': 947,\n 'deepe': 948,\n 'mock': 949,\n 'hard': 950,\n 'tables': 951,\n 'goodly': 952,\n 'admiration': 953,\n 'deliuer': 954,\n 'cap': 955,\n 'appeares': 956,\n 'whilst': 957,\n 'dreadfull': 958,\n 'third': 959,\n 'sirs': 960,\n 'foote': 961,\n 'staid': 962,\n 'assume': 963,\n 'person': 964,\n 'mens': 965,\n 'choyce': 966,\n 'beauty': 967,\n 'spring': 968,\n 'aboord': 969,\n 'saile': 970,\n 'few': 971,\n 'steele': 972,\n 'entertainment': 973,\n 'censure': 974,\n 'rich': 975,\n 'generous': 976,\n 'edge': 977,\n 'tenders': 978,\n 'pay': 979,\n 'tender': 980,\n 'honourable': 981,\n 'presence': 982,\n 'meere': 983,\n 'wayes': 984,\n 'hower': 985,\n 'strooke': 986,\n 'wont': 987,\n \"honour'd\": 988,\n 'angels': 989,\n 'royall': 990,\n 'burst': 991,\n 'ignorance': 992,\n 'shake': 993,\n 'wherefore': 994,\n 'tempt': 995,\n 'lets': 996,\n 'lead': 997,\n 'crimes': 998,\n 'starres': 999,\n 'stirre': 1000,\n ...}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"### creating input sequence\ninput_sequences = []\nfor line in text.split('\\n'):\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1,len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:40:39.213638Z","iopub.execute_input":"2025-07-08T09:40:39.214220Z","iopub.status.idle":"2025-07-08T09:40:39.492434Z","shell.execute_reply.started":"2025-07-08T09:40:39.214199Z","shell.execute_reply":"2025-07-08T09:40:39.491875Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"input_sequences[0:20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:40:58.934182Z","iopub.execute_input":"2025-07-08T09:40:58.934442Z","iopub.status.idle":"2025-07-08T09:40:58.939975Z","shell.execute_reply.started":"2025-07-08T09:40:58.934420Z","shell.execute_reply":"2025-07-08T09:40:58.939331Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[[1, 687],\n [1, 687, 4],\n [1, 687, 4, 45],\n [1, 687, 4, 45, 41],\n [1, 687, 4, 45, 41, 1886],\n [1, 687, 4, 45, 41, 1886, 1887],\n [1, 687, 4, 45, 41, 1886, 1887, 1888],\n [1180, 1889],\n [1180, 1889, 1890],\n [1180, 1889, 1890, 1891],\n [57, 407],\n [57, 407, 2],\n [57, 407, 2, 1181],\n [57, 407, 2, 1181, 177],\n [57, 407, 2, 1181, 177, 1892],\n [407, 1182],\n [407, 1182, 63],\n [408, 162],\n [408, 162, 377],\n [408, 162, 377, 21]]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"## padding the sequences\nmax_sequence_length = max([len(x) for x in input_sequences])\nmax_sequence_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:42:11.623529Z","iopub.execute_input":"2025-07-08T09:42:11.624233Z","iopub.status.idle":"2025-07-08T09:42:11.631451Z","shell.execute_reply.started":"2025-07-08T09:42:11.624208Z","shell.execute_reply":"2025-07-08T09:42:11.630914Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"14"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"input_sequences = np.array(pad_sequences(input_sequences,maxlen = max_sequence_length,padding = 'pre'))\ninput_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:43:25.934063Z","iopub.execute_input":"2025-07-08T09:43:25.934356Z","iopub.status.idle":"2025-07-08T09:43:25.987930Z","shell.execute_reply.started":"2025-07-08T09:43:25.934333Z","shell.execute_reply":"2025-07-08T09:43:25.987332Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0, ...,    0,    1,  687],\n       [   0,    0,    0, ...,    1,  687,    4],\n       [   0,    0,    0, ...,  687,    4,   45],\n       ...,\n       [   0,    0,    0, ...,    4,   45, 1047],\n       [   0,    0,    0, ...,   45, 1047,    4],\n       [   0,    0,    0, ..., 1047,    4,  193]], dtype=int32)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"##creating input and label\n\nimport tensorflow as tf\nx,y = input_sequences[:,:-1],input_sequences[:,-1]\n\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:44:42.853777Z","iopub.execute_input":"2025-07-08T09:44:42.854357Z","iopub.status.idle":"2025-07-08T09:44:42.859335Z","shell.execute_reply.started":"2025-07-08T09:44:42.854335Z","shell.execute_reply":"2025-07-08T09:44:42.858686Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([ 687,    4,   45, ..., 1047,    4,  193], dtype=int32)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(y,num_classes = total_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:45:20.399724Z","iopub.execute_input":"2025-07-08T09:45:20.399987Z","iopub.status.idle":"2025-07-08T09:45:20.476863Z","shell.execute_reply.started":"2025-07-08T09:45:20.399967Z","shell.execute_reply":"2025-07-08T09:45:20.476306Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:45:53.603684Z","iopub.execute_input":"2025-07-08T09:45:53.603948Z","iopub.status.idle":"2025-07-08T09:45:54.200965Z","shell.execute_reply.started":"2025-07-08T09:45:53.603930Z","shell.execute_reply":"2025-07-08T09:45:54.200413Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n#Define the model\nmodel=Sequential()\nmodel.add(Embedding(total_words,100,input_length = max_sequence_length))\nmodel.add(LSTM(128,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dense(total_words,activation=\"softmax\"))\n\n##compiling\nmodel.build(input_shape=(None, max_sequence_length))\nmodel.compile(loss = \"categorical_crossentropy\",optimizer = \"adam\",metrics = ['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:56:52.904320Z","iopub.execute_input":"2025-07-08T09:56:52.904638Z","iopub.status.idle":"2025-07-08T09:56:55.387497Z","shell.execute_reply.started":"2025-07-08T09:56:52.904615Z","shell.execute_reply":"2025-07-08T09:56:55.386712Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:57:18.894432Z","iopub.execute_input":"2025-07-08T09:57:18.895170Z","iopub.status.idle":"2025-07-08T09:57:18.910016Z","shell.execute_reply.started":"2025-07-08T09:57:18.895143Z","shell.execute_reply":"2025-07-08T09:57:18.909362Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m481,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m117,248\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4818\u001b[0m)           │       \u001b[38;5;34m621,522\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">481,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4818</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">621,522</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,352,154\u001b[0m (5.16 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,352,154</span> (5.16 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,352,154\u001b[0m (5.16 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,352,154</span> (5.16 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"##training\nhistory = model.fit(x_train,y_train,epochs = 80,validation_data = (x_test,y_test),verbose = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:02:29.104961Z","iopub.execute_input":"2025-07-08T10:02:29.105597Z","iopub.status.idle":"2025-07-08T10:09:13.441912Z","shell.execute_reply.started":"2025-07-08T10:02:29.105572Z","shell.execute_reply":"2025-07-08T10:09:13.441011Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/80\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1751968952.543086     129 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.0313 - loss: 7.1072 - val_accuracy: 0.0367 - val_loss: 6.6983\nEpoch 2/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0382 - loss: 6.4531 - val_accuracy: 0.0447 - val_loss: 6.7727\nEpoch 3/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0429 - loss: 6.2981 - val_accuracy: 0.0507 - val_loss: 6.8313\nEpoch 4/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0505 - loss: 6.1517 - val_accuracy: 0.0513 - val_loss: 6.8552\nEpoch 5/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0557 - loss: 5.9927 - val_accuracy: 0.0538 - val_loss: 6.8844\nEpoch 6/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0627 - loss: 5.8554 - val_accuracy: 0.0596 - val_loss: 6.9220\nEpoch 7/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0678 - loss: 5.7101 - val_accuracy: 0.0618 - val_loss: 6.9819\nEpoch 8/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0736 - loss: 5.5946 - val_accuracy: 0.0663 - val_loss: 7.0422\nEpoch 9/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0787 - loss: 5.4616 - val_accuracy: 0.0715 - val_loss: 7.1171\nEpoch 10/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0837 - loss: 5.3181 - val_accuracy: 0.0668 - val_loss: 7.2381\nEpoch 11/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0911 - loss: 5.1796 - val_accuracy: 0.0680 - val_loss: 7.3483\nEpoch 12/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0989 - loss: 5.0337 - val_accuracy: 0.0686 - val_loss: 7.4167\nEpoch 13/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1029 - loss: 4.9123 - val_accuracy: 0.0707 - val_loss: 7.5350\nEpoch 14/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1058 - loss: 4.7730 - val_accuracy: 0.0713 - val_loss: 7.6646\nEpoch 15/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.1134 - loss: 4.6497 - val_accuracy: 0.0668 - val_loss: 7.7917\nEpoch 16/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1254 - loss: 4.5120 - val_accuracy: 0.0688 - val_loss: 7.9565\nEpoch 17/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1283 - loss: 4.4203 - val_accuracy: 0.0664 - val_loss: 8.0474\nEpoch 18/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.1519 - loss: 4.2381 - val_accuracy: 0.0639 - val_loss: 8.2338\nEpoch 19/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1614 - loss: 4.1774 - val_accuracy: 0.0661 - val_loss: 8.3918\nEpoch 20/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1723 - loss: 4.0739 - val_accuracy: 0.0651 - val_loss: 8.4886\nEpoch 21/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1911 - loss: 3.9846 - val_accuracy: 0.0604 - val_loss: 8.6655\nEpoch 22/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.2060 - loss: 3.8791 - val_accuracy: 0.0628 - val_loss: 8.8128\nEpoch 23/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.2266 - loss: 3.7589 - val_accuracy: 0.0606 - val_loss: 8.9122\nEpoch 24/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.2399 - loss: 3.6893 - val_accuracy: 0.0633 - val_loss: 9.0507\nEpoch 25/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.2542 - loss: 3.5724 - val_accuracy: 0.0591 - val_loss: 9.1650\nEpoch 26/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.2654 - loss: 3.5262 - val_accuracy: 0.0604 - val_loss: 9.3084\nEpoch 27/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.2823 - loss: 3.4386 - val_accuracy: 0.0573 - val_loss: 9.3979\nEpoch 28/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.2977 - loss: 3.3566 - val_accuracy: 0.0565 - val_loss: 9.5022\nEpoch 29/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3024 - loss: 3.2950 - val_accuracy: 0.0589 - val_loss: 9.6317\nEpoch 30/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3190 - loss: 3.2164 - val_accuracy: 0.0554 - val_loss: 9.7515\nEpoch 31/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.3320 - loss: 3.1515 - val_accuracy: 0.0569 - val_loss: 9.8494\nEpoch 32/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3340 - loss: 3.1075 - val_accuracy: 0.0538 - val_loss: 9.9607\nEpoch 33/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3521 - loss: 3.0206 - val_accuracy: 0.0530 - val_loss: 10.0291\nEpoch 34/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3611 - loss: 2.9799 - val_accuracy: 0.0556 - val_loss: 10.1471\nEpoch 35/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3747 - loss: 2.9016 - val_accuracy: 0.0513 - val_loss: 10.2198\nEpoch 36/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3741 - loss: 2.8929 - val_accuracy: 0.0528 - val_loss: 10.3367\nEpoch 37/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3823 - loss: 2.8474 - val_accuracy: 0.0561 - val_loss: 10.4084\nEpoch 38/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.3944 - loss: 2.7862 - val_accuracy: 0.0567 - val_loss: 10.5008\nEpoch 39/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4097 - loss: 2.7263 - val_accuracy: 0.0525 - val_loss: 10.5792\nEpoch 40/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4138 - loss: 2.6877 - val_accuracy: 0.0536 - val_loss: 10.6596\nEpoch 41/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4188 - loss: 2.6405 - val_accuracy: 0.0521 - val_loss: 10.7719\nEpoch 42/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4321 - loss: 2.5882 - val_accuracy: 0.0536 - val_loss: 10.8127\nEpoch 43/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4409 - loss: 2.5497 - val_accuracy: 0.0536 - val_loss: 10.8812\nEpoch 44/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4433 - loss: 2.5099 - val_accuracy: 0.0532 - val_loss: 11.0085\nEpoch 45/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4519 - loss: 2.4920 - val_accuracy: 0.0527 - val_loss: 11.0586\nEpoch 46/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4602 - loss: 2.4394 - val_accuracy: 0.0515 - val_loss: 11.1281\nEpoch 47/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4719 - loss: 2.4037 - val_accuracy: 0.0517 - val_loss: 11.1715\nEpoch 48/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4795 - loss: 2.3412 - val_accuracy: 0.0538 - val_loss: 11.2573\nEpoch 49/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4865 - loss: 2.3184 - val_accuracy: 0.0509 - val_loss: 11.3405\nEpoch 50/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4918 - loss: 2.2961 - val_accuracy: 0.0525 - val_loss: 11.4149\nEpoch 51/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5052 - loss: 2.2453 - val_accuracy: 0.0505 - val_loss: 11.4821\nEpoch 52/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5041 - loss: 2.2314 - val_accuracy: 0.0548 - val_loss: 11.5865\nEpoch 53/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5066 - loss: 2.2089 - val_accuracy: 0.0505 - val_loss: 11.6308\nEpoch 54/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5199 - loss: 2.1439 - val_accuracy: 0.0511 - val_loss: 11.6969\nEpoch 55/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5276 - loss: 2.1180 - val_accuracy: 0.0509 - val_loss: 11.7741\nEpoch 56/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5266 - loss: 2.1016 - val_accuracy: 0.0521 - val_loss: 11.8022\nEpoch 57/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5369 - loss: 2.0681 - val_accuracy: 0.0509 - val_loss: 11.9011\nEpoch 58/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5367 - loss: 2.0519 - val_accuracy: 0.0495 - val_loss: 11.9473\nEpoch 59/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5457 - loss: 2.0052 - val_accuracy: 0.0534 - val_loss: 12.0004\nEpoch 60/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5528 - loss: 1.9839 - val_accuracy: 0.0511 - val_loss: 12.1070\nEpoch 61/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5545 - loss: 1.9736 - val_accuracy: 0.0536 - val_loss: 12.1445\nEpoch 62/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5688 - loss: 1.9180 - val_accuracy: 0.0511 - val_loss: 12.2122\nEpoch 63/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5734 - loss: 1.9116 - val_accuracy: 0.0507 - val_loss: 12.2626\nEpoch 64/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5772 - loss: 1.8750 - val_accuracy: 0.0495 - val_loss: 12.3124\nEpoch 65/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5779 - loss: 1.8658 - val_accuracy: 0.0527 - val_loss: 12.4145\nEpoch 66/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5906 - loss: 1.8188 - val_accuracy: 0.0499 - val_loss: 12.4409\nEpoch 67/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5976 - loss: 1.7877 - val_accuracy: 0.0552 - val_loss: 12.4871\nEpoch 68/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5948 - loss: 1.7804 - val_accuracy: 0.0530 - val_loss: 12.5558\nEpoch 69/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6015 - loss: 1.7521 - val_accuracy: 0.0530 - val_loss: 12.5940\nEpoch 70/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6087 - loss: 1.7317 - val_accuracy: 0.0493 - val_loss: 12.6676\nEpoch 71/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6128 - loss: 1.6990 - val_accuracy: 0.0501 - val_loss: 12.7338\nEpoch 72/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6169 - loss: 1.6751 - val_accuracy: 0.0505 - val_loss: 12.7959\nEpoch 73/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6252 - loss: 1.6586 - val_accuracy: 0.0505 - val_loss: 12.8554\nEpoch 74/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6253 - loss: 1.6504 - val_accuracy: 0.0515 - val_loss: 12.8553\nEpoch 75/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6265 - loss: 1.6244 - val_accuracy: 0.0532 - val_loss: 12.9680\nEpoch 76/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6325 - loss: 1.6083 - val_accuracy: 0.0519 - val_loss: 12.9895\nEpoch 77/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6350 - loss: 1.5903 - val_accuracy: 0.0507 - val_loss: 13.0775\nEpoch 78/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6384 - loss: 1.5711 - val_accuracy: 0.0525 - val_loss: 13.0933\nEpoch 79/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6424 - loss: 1.5517 - val_accuracy: 0.0499 - val_loss: 13.1813\nEpoch 80/80\n\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6461 - loss: 1.5291 - val_accuracy: 0.0492 - val_loss: 13.2295\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef predict_next_word(model, tokenizer, text, max_sequence_length):\n    # Convert input text to token list\n    token_list = tokenizer.texts_to_sequences([text])[0]\n\n    # Ensure the token list is of the correct length\n    if len(token_list) >= max_sequence_length:\n        token_list = token_list[-(max_sequence_length - 1):]  # leave space for next word\n\n    # Pad the sequence\n    token_list = pad_sequences([token_list], maxlen=max_sequence_length, padding='pre')\n\n    # Predict the next word\n    predicted_probs = model.predict(token_list, verbose=0)\n    predicted_word_index = np.argmax(predicted_probs, axis=1)[0]  # Get scalar from array\n\n    # Map index back to word\n    for word, index in tokenizer.word_index.items():\n        if index == predicted_word_index:\n            return word\n\n    return None  # In case the word wasn't found\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:17:41.920517Z","iopub.execute_input":"2025-07-08T10:17:41.920873Z","iopub.status.idle":"2025-07-08T10:17:41.926276Z","shell.execute_reply.started":"2025-07-08T10:17:41.920853Z","shell.execute_reply":"2025-07-08T10:17:41.925521Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"input_text = \"To be or not to be\"\nprint(f\"Input_text :{input_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:18:57.964123Z","iopub.execute_input":"2025-07-08T10:18:57.964394Z","iopub.status.idle":"2025-07-08T10:18:57.968602Z","shell.execute_reply.started":"2025-07-08T10:18:57.964373Z","shell.execute_reply":"2025-07-08T10:18:57.967762Z"}},"outputs":[{"name":"stdout","text":"Input_text :To be or not to be\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"max_sequence_len = model.input_shape[1]\nnext_word = predict_next_word(model,tokenizer,input_text,max_sequence_len)\nprint(f\"next word prediction :{next_word}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:38:37.238824Z","iopub.execute_input":"2025-07-08T10:38:37.239572Z","iopub.status.idle":"2025-07-08T10:38:37.322957Z","shell.execute_reply.started":"2025-07-08T10:38:37.239544Z","shell.execute_reply":"2025-07-08T10:38:37.322254Z"}},"outputs":[{"name":"stdout","text":"next word prediction :as\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"model.save(\"next_word_lstm.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:38:58.669673Z","iopub.execute_input":"2025-07-08T10:38:58.670367Z","iopub.status.idle":"2025-07-08T10:38:58.758044Z","shell.execute_reply.started":"2025-07-08T10:38:58.670341Z","shell.execute_reply":"2025-07-08T10:38:58.757490Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"##save the tokenizer\nimport pickle\nwith open('tokenizer.pickle','wb') as handle:\n pickle.dump(tokenizer,handle,protocol = pickle.HIGHEST_PROTOCOL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:41:25.929018Z","iopub.execute_input":"2025-07-08T10:41:25.929720Z","iopub.status.idle":"2025-07-08T10:41:25.936745Z","shell.execute_reply.started":"2025-07-08T10:41:25.929696Z","shell.execute_reply":"2025-07-08T10:41:25.936148Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null}]}